{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using Byte-Pair Encoding and a Unigram Language Model\n",
    "\n",
    "Author: Pierre Nugues with help from Marcus Klang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a tokenization program to handle subwords.\n",
    "\n",
    "In many scripts from Asia, like Chinese, Korean, or Japanese scripts, tokenization cannot rely on white spaces. The byte-pair encoding and the unigram language model are techniques that are now common in machine translation to carry out a tokenization at a subword level. Subword level tokenization shows better multilingual capabilities.\n",
    "\n",
    "You will follow two papers: \n",
    "* Subword Regularization: _Improving Neural Network Translation Models with Multiple Subword Candidates_ by Kudo (2018) (https://arxiv.org/pdf/1804.10959.pdf) and \n",
    "* _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ by Bostrom and Durrett (2020) (https://aclanthology.org/2020.findings-emnlp.414.pdf). \n",
    "\n",
    "In addition, you will start from a clear and easy-to-understand description in Google’s Neural Machine Translation System: _Bridging the Gap between Human and Machine Translation_ by Wu et al. (2016). (Do not read them now)\n",
    "https://arxiv.org/abs/1609.08144\n",
    "\n",
    "You will use a small corpus make it easier to test and correct your code. Note also that you will use _characters_ and not _bytes_ in this lab as this is simpler to implement. For a complete program, see the link at the end.\n",
    "\n",
    "**In your report, be sure to answer all the questions. Please reuse the section titles of this notebook so that I can check your answers more easily**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an overall description of the subword tokenizers, read Sections 4 (introduction paragraph) and 4.1. in the paper on translation: _Bridging the Gap between Human and Machine Translation_ by Wu et al. (2016), https://arxiv.org/abs/1609.08144.  \n",
    "\n",
    "In your report, in a few lines (10 to 15 lines or so) you will:\n",
    "\n",
    "1. Outline the difference with tokenization as you saw it during the course;\n",
    "2. Imagine how the tokens will be learned (this will developed in the rest of the lab);\n",
    "3. Summarize what could be the advantages for Asian languages, unknown words, and translation.\n",
    "\n",
    "Commenting Sections 4 and 4.1 in your report is **mandatory**. If you are curious, you can read the complete article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the BPE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm to build the subwords from a corpus is a byte-pair encoding (BPE), due to Gage (1994). In the lab, you will first read two sections of more recent articles as they are easier to understand and specifically targeted to natural language processing.\n",
    "\n",
    "Read these two sections:\n",
    "\n",
    "1. Section 3.1 of _Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates_ (https://arxiv.org/pdf/1804.10959.pdf) by Kudo (2018).\n",
    "2. Section 2, algorithm 1 of _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ (https://aclanthology.org/2020.findings-emnlp.414.pdf) by Bostrom and Durrett (2020).\n",
    "\n",
    "In your report, **summarize** (10 to 15 lines or so) with your own words the byte-pair encoding (BPE) algorithm as described by Kudo (2018) and Bostrom and Durrett (2020) (Only BPE and not the unigram language model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now program a byte-pair encoding program in Python. You will do it step by step. The first part will be to extract the subwords from a corpus. Note that you will use the characters, not the bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import tqdm as tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use a small corpus and then, if you have time, test your program on a larger one. Here we take the smallest novel from Selma Lagerlöf in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selma has been downloaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Selma/bannlyst.txt',\n",
       " 'Selma/gosta.txt',\n",
       " 'Selma/herrgard.txt',\n",
       " 'Selma/jerusalem.txt',\n",
       " 'Selma/kejsaren.txt',\n",
       " 'Selma/marbacka.txt',\n",
       " 'Selma/nils.txt',\n",
       " 'Selma/osynliga.txt',\n",
       " 'Selma/troll.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "# Parameters for Selma dataset\n",
    "SELMA_URL = \"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\"\n",
    "\n",
    "SELMA_FILES = [\n",
    "    os.path.join(\"Selma\", fname) \n",
    "    for fname in \n",
    "    [\n",
    "        \"bannlyst.txt\", \n",
    "        \"gosta.txt\", \n",
    "        \"herrgard.txt\", \n",
    "        \"jerusalem.txt\", \n",
    "        \"kejsaren.txt\", \n",
    "        \"marbacka.txt\", \n",
    "        \"nils.txt\", \n",
    "        \"osynliga.txt\", \n",
    "        \"troll.txt\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "def download_and_extract_selma():\n",
    "    \"\"\"Downloads and unpacks Selma.zip\"\"\"\n",
    "    \n",
    "    # Download if not all files exist\n",
    "    req = requests.get(SELMA_URL, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Failed to download file, got status: \" + req.status_code)\n",
    "        req.close()\n",
    "    else:\n",
    "        with open(\"Selma.zip\", \"wb\") as fd:\n",
    "            written = 0\n",
    "            for chunk in req.iter_content(chunk_size=65536):\n",
    "                fd.write(chunk)\n",
    "                written += len(chunk)\n",
    "                print(\"Downloading: %d bytes written to Selma.zip\" % written)\n",
    "\n",
    "        print(\"Selma.zip donwnloaded.\")\n",
    "        req.close()\n",
    "        \n",
    "        selma_zipfile = ZipFile(\"Selma.zip\")\n",
    "        selma_files_to_extract = [zi for zi in selma_zipfile.filelist if not zi.filename.startswith(\"__\") and zi.filename.endswith(\".txt\")]\n",
    "        for zi in selma_files_to_extract:\n",
    "            selma_zipfile.extract(zi)\n",
    "            print(\"Extracted: \" + zi.filename)\n",
    "            \n",
    "        print(\"Done!\")\n",
    "        \n",
    "# If not all path exists (all are true), then download\n",
    "if not all([os.path.exists(fname) for fname in SELMA_FILES]):\n",
    "    download_and_extract_selma()\n",
    "else:\n",
    "    print(\"Selma has been downloaded.\")\n",
    "    \n",
    "SELMA_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE_PATH = '../../corpus/Selma.txt'\n",
    "FILE_PATH = '/Users/filippahansen/Desktop/Språkteknologi/edan20-rep/notebooks/Selma/herrgard.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the corpus and store it in the `corpus` string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, encoding='utf8') as f:\n",
    "    corpus = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all the space sequences in `corpus`, including newlines and tabulations, and normalize them as one space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = re.sub(r'\\s+', ' ', corpus) # \\s is [\\t\\n\\x0b\\r\\f]\n",
    "#corpus = re.sub(r'\\n', ' ', corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selma Lagerlöf En herrgårdssägen Bokutgåva Albert Bonniers förlag, Stockholm 1899. I. Det var en skö\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code (one instruction) to split the corpus in a list of characters and store the results in `corpus_l`. This is just a type conversion. Given the input:\n",
    "<pre><span style=\"font-size: 12pt;\">corpus = 'De senaste fem &aring;ren har cirka 25 000 unga'</span></pre>\n",
    "\n",
    "Return:\n",
    "<pre><span style=\"font-size: 12pt;\">corpus_l = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', 'e', ' ', 'f', 'e', 'm', ' ', ...]</span></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_l = list(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'e', 'l', 'm', 'a', ' ', 'L', 'a', 'g', 'e', 'r', 'l', 'ö', 'f', ' ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_l[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the set of characters that will serve as initial subword tokens:\n",
    "\n",
    "1. Write a statement to extract the set of all the characters from `corpus_l`; \n",
    "2. Exclude the space from this set and call the resulting set: `char_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = set(corpus_l)\n",
    "char_set.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from the previous question, write an `initial_vocabulary()` function taking the the `corpus_l` variable as input and returning the the set of all characters appearing in the corpus (the initial character set), deprived from the white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_vocabulary(corpus_l):\n",
    "    char_set = set(corpus_l)\n",
    "    if ' ' in char_set:\n",
    "        char_set.remove(' ')\n",
    "    return list(sorted(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ',', '-', '.', '1', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'X', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '»', 'Ä', 'Å', 'Ö', 'ä', 'å', 'é', 'ö', '–', '’']\n"
     ]
    }
   ],
   "source": [
    "print(initial_vocabulary(corpus_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `pair_count()` function that takes a list of tokens as input, possibly single characters or subword tokens, and that counts the adjacent pairs (bigrams). You will implement these counts as dictionaries: The key will be a pair (tuple) of adjacent symbols and the value, its frequency. Remember that you cannot cross whitespaces, i.e. a pair cannot include a whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input\n",
    "\n",
    "`['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', ...]`\n",
    "count_pairs should return a dictionary: \n",
    "\n",
    "\n",
    "`{('D', 'e'): 1, ('s', 'e'): 1, ('e', 'n'): 1, ('n', 'a'): 1, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_count(tokens):\n",
    "    #for x in tokens:\n",
    "    #    if x == ' ':\n",
    "    #        tokens.remove(x)\n",
    "    bigrams = [tuple(tokens[idx:idx + 2])\n",
    "               for idx in range(len(tokens) - 1)]\n",
    "    pairs = {}\n",
    "    for bigram in bigrams:\n",
    "        #if bigram[0] == '.' or bigram[1]=='.' or bigram[0] == ',' or bigram[1]==',':\n",
    "         #   continue\n",
    "        if bigram[0] == ' ' or bigram[1] == ' ':\n",
    "            continue\n",
    "        if ((bigram[0],bigram[1])) in pairs:\n",
    "            pairs[(bigram[0],bigram[1])] += 1\n",
    "        else:\n",
    "            pairs[(bigram[0],bigram[1])] = 1\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pairs = pair_count(corpus_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('S', 'e'): 7, ('e', 'l'): 511, ('l', 'm'): 22, ('m', 'a'): 424, ('L', 'a'): 8, ('a', 'g'): 477, ('g', 'e'): 646, ('e', 'r'): 1281, ('r', 'l'): 213, ('l', 'ö'): 70, ('ö', 'f'): 217, ('E', 'n'): 21, ('h', 'e'): 773, ('r', 'r'): 177, ('r', 'g'): 140, ('g', 'å'): 327, ('å', 'r'): 295, ('r', 'd'): 413, ('d', 's'): 83, ('s', 's'): 218, ('s', 'ä'): 205, ('ä', 'g'): 143, ('e', 'n'): 3439, ('B', 'o'): 2, ('o', 'k'): 54, ('k', 'u'): 562, ('u', 't'): 295, ('t', 'g'): 43, ('å', 'v'): 2, ('v', 'a'): 1426, ('A', 'l'): 24, ('l', 'b'): 44, ('b', 'e'): 314, ('r', 't'): 381, ('o', 'n'): 1600, ('n', 'n'): 1070, ('n', 'i'): 231, ('i', 'e'): 22, ('r', 's'): 258, ('f', 'ö'): 980, ('ö', 'r'): 1317, ('l', 'a'): 873, ('g', ','): 192, ('S', 't'): 88, ('t', 'o'): 351, ('o', 'c'): 1223, ('c', 'k'): 794, ('k', 'h'): 23, ('h', 'o'): 1098, ('o', 'l'): 211, ('1', '8'): 1, ('8', '9'): 1, ('9', '9'): 1, ('9', '.'): 1, ('I', '.'): 6, ('D', 'e'): 429, ('e', 't'): 1954, ('a', 'r'): 1970, ('s', 'k'): 835, ('k', 'ö'): 57, ('ö', 'n'): 101, ('h', 'ö'): 223, ('ö', 's'): 77, ('s', 't'): 1632, ('t', 'd'): 3, ('d', 'a'): 626, ('h', 'ä'): 194, ('ä', 'n'): 741, ('n', 'e'): 813, ('e', 'm'): 213, ('m', 'o'): 208, ('o', 't'): 255, ('s', 'l'): 162, ('l', 'u'): 75, ('t', 'e'): 1102, ('a', 'f'): 419, ('t', 'r'): 379, ('r', 'e'): 526, ('t', 't'): 2178, ('t', 'i'): 676, ('i', 'o'): 95, ('t', 'a'): 1180, ('a', 'l'): 722, ('l', 'e'): 887, ('t', '.'): 304, ('P', 'å'): 14, ('d', 'e'): 4016, ('i', 'd'): 484, ('f', 'a'): 194, ('a', 'n'): 2441, ('n', 's'): 453, ('U', 'p'): 9, ('p', 's'): 29, ('s', 'a'): 644, ('ö', 'g'): 125, ('g', 't'): 287, ('t', ','): 409, ('g', 'u'): 32, ('u', 'l'): 353, ('l', 't'): 214, ('t', 'v'): 67, ('v', 'å'): 94, ('å', 'n'): 325, ('i', 'n'): 1047, ('n', 'g'): 1119, ('g', 's'): 109, ('s', 'h'): 10, ('h', 'u'): 160, ('u', 's'): 158, ('s', ','): 45, ('s', 'o'): 632, ('o', 'm'): 1610, ('o', 'd'): 220, ('u', 'n'): 671, ('n', 'd'): 1063, ('l', 'i'): 908, ('i', 'g'): 1139, ('a', 'm'): 353, ('m', 't'): 72, ('p', 'å'): 522, ('i', 't'): 506, ('l', 'å'): 190, ('b', 'o'): 161, ('o', 'r'): 739, ('t', 'k'): 8, ('k', 'a'): 831, ('n', 't'): 331, ('a', 'd'): 1441, ('n', '.'): 485, ('r', 'ä'): 368, ('ä', 't'): 180, ('r', 'u'): 333, ('k', 'i'): 109, ('c', 'h'): 1234, ('e', 'f'): 232, ('f', 'l'): 132, ('m', 'e'): 739, ('n', 'a'): 1027, ('e', 's'): 406, ('a', 's'): 292, ('v', 'i'): 562, ('i', 'l'): 642, ('l', 'd'): 218, ('d', 'v'): 39, ('n', ','): 512, ('v', 'ä'): 357, ('ä', 'x'): 17, ('x', 't'): 9, ('d', 'ä'): 206, ('ä', 'r'): 819, ('r', ','): 244, ('l', 's'): 182, ('s', 'i'): 625, ('k', 'r'): 190, ('ä', 'l'): 348, ('s', 'å'): 595, ('u', 'p'): 197, ('p', 'p'): 322, ('p', 'f'): 13, ('g', 'g'): 74, ('a', 't'): 1639, ('l', 'l'): 1551, ('m', 'r'): 16, ('r', 'a'): 1079, ('f', 'v'): 564, ('v', 'e'): 454, ('r', 'v'): 36, ('u', 'm'): 88, ('n', 'f'): 24, ('n', 'r'): 9, ('t', 'u'): 123, ('u', 'd'): 84, ('d', 'r'): 263, ('a', 'c'): 77, ('g', 'o'): 359, ('n', 'k'): 188, ('f', 'f'): 39, ('f', 'e'): 17, ('e', '.'): 276, ('H', 'a'): 246, ('k', 'e'): 460, ('e', 'd'): 680, ('f', 'i'): 176, ('t', 's'): 127, ('s', 'e'): 298, ('e', 'e'): 21, ('H', 'å'): 1, ('b', 'a'): 231, ('h', 'a'): 1366, ('u', 'k'): 89, ('u', 'r'): 207, ('p', 'a'): 215, ('u', 'g'): 62, ('t', 'ä'): 197, ('d', 'i'): 216, ('k', 'l'): 133, ('l', 'ä'): 238, ('ä', 'd'): 173, ('d', 'd'): 155, ('e', 'k'): 74, ('k', 'v'): 63, ('ä', 'm'): 113, ('ä', 'k'): 37, ('k', 't'): 379, ('e', 'g'): 107, ('g', 'a'): 611, ('m', ','): 133, ('o', 'f'): 75, ('a', ','): 363, ('o', 'p'): 66, ('r', 'i'): 613, ('i', 'f'): 205, ('f', 'b'): 6, ('p', 'r'): 96, ('h', 'y'): 29, ('y', 'l'): 15, ('l', 'o'): 233, ('n', 'ä'): 124, ('ä', 's'): 198, ('b', 'ö'): 96, ('ö', 'c'): 2, ('r', '.'): 179, ('I', 'n'): 272, ('i', 'c'): 254, ('e', ','): 315, ('k', 'o'): 491, ('n', 'o'): 403, ('m', '.'): 156, ('r', 'b'): 50, ('b', 'r'): 167, ('l', ','): 62, ('r', 'k'): 225, ('k', ','): 55, ('f', 'u'): 45, ('i', 'k'): 182, ('h', 'å'): 89, ('g', 'r'): 496, ('r', 'o'): 290, ('y', '.'): 1, ('D', 'u'): 29, ('u', ','): 39, ('H', 'e'): 142, ('j', 'a'): 286, ('m', 'm'): 419, ('l', 'v'): 10, ('g', '.'): 168, ('d', 'u'): 163, ('r', 'å'): 253, ('å', 'k'): 42, ('n', 'å'): 234, ('å', 'g'): 453, ('o', 'b'): 14, ('e', 'h'): 56, ('g', 'l'): 104, ('t', '?'): 23, ('e', 'j'): 591, ('j', ','): 39, ('s', 'n'): 95, ('g', 'ä'): 43, ('t', 'y'): 171, ('y', 's'): 68, ('d', '.'): 118, ('L', 'å'): 7, ('å', 't'): 296, ('d', 'å'): 228, ('o', 'g'): 200, ('b', 'l'): 365, ('t', 'l'): 50, ('l', 'y'): 94, ('d', 'l'): 48, ('n', 'u'): 148, ('J', 'a'): 96, ('m', 'i'): 220, ('t', 'å'): 107, ('u', '.'): 15, ('s', 'j'): 115, ('j', 'ä'): 177, ('l', 'f'): 80, ('f', ':'): 1, ('G', 'u'): 41, ('Å', 'l'): 25, ('r', 'p'): 17, ('y', 'c'): 215, ('å', 'd'): 189, ('m', 'ä'): 107, ('F', 'ö'): 17, ('M', 'i'): 8, ('r', 'f'): 136, ('j', 'u'): 270, ('i', 'h'): 30, ('r', 'ö'): 189, ('f', 'r'): 419, ('m', 'f'): 27, ('h', 'v'): 202, ('r', 'j'): 107, ('j', 'e'): 54, ('n', 'b'): 44, ('t', 'm'): 25, ('h', 'j'): 53, ('l', 'p'): 37, ('n', 'h'): 10, ('l', 'k'): 95, ('l', 'n'): 50, ('p', 't'): 49, ('f', 'å'): 130, ('a', '.'): 265, ('r', 'y'): 81, ('T', 'a'): 8, ('k', 'y'): 103, ('y', 'm'): 27, ('g', 'ö'): 67, ('k', 'n'): 75, ('a', 'p'): 73, ('ö', 'p'): 35, ('p', 'n'): 29, ('f', 'y'): 21, ('y', 'r'): 93, ('r', 'm'): 96, ('r', 'n'): 341, ('a', 'k'): 151, ('s', 'p'): 152, ('p', 'e'): 169, ('å', 'l'): 104, ('g', 'i'): 147, ('F', 'a'): 5, ('v', 'u'): 43, ('l', 'r'): 22, ('f', 'o'): 116, ('t', 'f'): 41, ('m', 'h'): 15, ('M', 'u'): 20, ('y', 't'): 71, ('l', '.'): 56, ('e', 'x'): 6, ('x', 'a'): 11, ('k', 's'): 68, ('f', '.'): 32, ('m', 'g'): 54, ('m', 'u'): 47, ('s', 'r'): 40, ('M', 'e'): 181, ('å', ','): 85, ('u', 'f'): 53, ('i', 's'): 303, ('m', 'd'): 35, ('d', 'ö'): 80, ('ö', 'd'): 95, ('d', ','): 168, ('B', 'e'): 15, ('e', 'a'): 6, ('s', '?'): 4, ('T', 'ä'): 3, ('f', 't'): 166, ('f', ','): 20, ('j', 'o'): 81, ('d', 'b'): 11, ('V', 'ä'): 3, ('m', 'l'): 67, ('u', 'b'): 22, ('b', 'b'): 15, ('d', 't'): 146, ('o', ','): 23, ('H', 'u'): 19, ('r', 'h'): 26, ('s', 'm'): 74, ('m', 'å'): 165, ('H', 'o'): 279, ('m', 'k'): 27, ('t', 'ö'): 29, ('å', 'e'): 15, ('l', 'j'): 90, ('d', 'o'): 83, ('f', 'ä'): 72, ('n', 'y'): 60, ('m', '!'): 8, ('S', 'å'): 41, ('b', 'i'): 39, ('V', 'i'): 27, ('i', 'p'): 22, ('ä', 'c'): 87, ('n', '!'): 13, ('h', 'i'): 32, ('n', 'v'): 12, ('O', 'c'): 138, ('k', '.'): 51, ('D', 'ä'): 24, ('k', '!'): 1, ('n', 'ö'): 51, ('ö', 'j'): 66, ('V', 'a'): 8, ('g', 'n'): 53, ('o', 'ä'): 10, ('n', '?'): 21, ('s', 'v'): 119, ('e', 'p'): 22, ('b', 'ä'): 68, ('i', 'r'): 25, ('p', '.'): 10, ('»', 'Å'): 1, ('g', 'j'): 50, ('H', 'v'): 38, ('t', 'j'): 49, ('l', '?'): 2, ('s', 'u'): 23, ('B', 'a'): 9, ('j', 'l'): 26, ('o', 's'): 115, ('s', 'b'): 20, ('N', 'å'): 7, ('d', 'j'): 65, ('m', 's'): 42, ('G', 'j'): 1, ('Å', 'j'): 2, ('v', 'o'): 90, ('m', 'ö'): 74, ('ö', 't'): 56, ('y', 'n'): 60, ('.', '.'): 12, ('s', 'y'): 59, ('ö', '.'): 6, ('k', 'ä'): 151, ('m', 'n'): 53, ('D', 'å'): 60, ('i', 'm'): 33, ('ö', 'l'): 71, ('b', 'u'): 30, ('e', 'b'): 12, ('A', 't'): 1, ('å', 's'): 89, ('g', 'd'): 59, ('f', 'd'): 45, ('v', 'r'): 12, ('p', ','): 18, ('j', 'ö'): 31, ('J', 'u'): 19, ('g', 'v'): 2, ('p', 'l'): 58, ('ä', 'p'): 33, ('m', 'v'): 16, ('d', 'm'): 7, ('m', 'j'): 8, ('å', '.'): 27, ('B', 'l'): 52, ('ä', 'f'): 52, ('y', 'd'): 48, ('F', 'l'): 10, ('N', 'u'): 23, ('F', 'r'): 17, ('d', 'g'): 28, ('F', 'i'): 5, ('g', 'm'): 3, ('s', 'f'): 22, ('l', '–'): 1, ('d', 'h'): 5, ('m', 'p'): 18, ('d', 'f'): 14, ('d', '?'): 4, ('å', 'f'): 5, ('s', 'c'): 11, ('k', 'j'): 7, ('U', 'n'): 12, ('i', 'b'): 4, ('g', 'k'): 7, ('p', 'u'): 7, ('m', 'y'): 74, ('y', 'g'): 48, ('p', 'h'): 4, ('A', 'k'): 1, ('ö', 'm'): 79, ('t', 'n'): 44, ('e', '!'): 6, ('d', 'k'): 12, ('y', 'k'): 3, ('t', 'b'): 13, ('K', 'o'): 5, ('K', 'a'): 18, ('c', 'i'): 6, ('j', 'd'): 37, ('S', 'j'): 1, ('s', 'g'): 4, ('l', 'h'): 18, ('s', '.'): 37, ('f', 's'): 19, ('e', 'v'): 11, ('d', 'n'): 22, ('O', 'm'): 26, ('o', 'a'): 3, ('M', 'a'): 19, ('C', 'i'): 1, ('r', '!'): 6, ('I', 'b'): 2, ('m', 'b'): 12, ('e', 'c'): 11, ('K', 'u'): 4, ('p', 'm'): 6, ('T', 'r'): 4, ('J', 'o'): 3, ('V', 'å'): 4, ('n', 'l'): 30, ('L', 'ä'): 4, ('y', ','): 1, ('p', 'i'): 30, ('t', '!'): 10, ('e', 'z'): 1, ('z', '?'): 1, (',', 'v'): 1, ('r', '?'): 11, ('Ä', 'r'): 7, ('i', ','): 5, ('f', 'k'): 8, ('e', '?'): 11, ('n', 'p'): 2, ('N', 'e'): 22, ('T', 'å'): 2, ('s', 'ö'): 61, ('ö', 'k'): 65, ('k', ';'): 1, ('o', 'h'): 6, ('N', 'ä'): 5, ('g', 'h'): 24, ('_', '_'): 119, ('I', 'I'): 6, ('ö', 'a'): 3, ('M', 'ä'): 3, ('f', 'n'): 16, ('y', 'p'): 3, ('u', 'x'): 9, ('x', 'i'): 3, ('b', 'y'): 10, ('Ö', 'f'): 1, ('p', 'g'): 2, ('n', 'm'): 5, ('a', 'v'): 7, ('ö', 'b'): 3, ('G', 'a'): 1, ('Ä', 'f'): 1, ('g', 'f'): 59, ('x', 'n'): 2, ('V', 'e'): 5, ('o', 'v'): 6, ('G', 'e'): 15, ('b', 'å'): 19, ('ö', ','): 7, ('y', 'f'): 29, ('a', 'x'): 8, ('x', 'l'): 3, ('N', 'i'): 7, ('o', 'j'): 3, ('H', 'ö'): 10, ('x', 'e'): 9, ('å', 'h'): 1, ('S', 'n'): 2, ('j', '.'): 10, ('R', 'å'): 10, ('D', 'a'): 6, ('O', 'v'): 1, ('b', 'j'): 11, ('S', 'y'): 2, ('k', 'd'): 5, ('L', 'i'): 16, ('G', 'r'): 5, ('k', 'f'): 5, ('G', 'o'): 2, ('l', '!'): 2, ('g', '?'): 8, ('a', '?'): 12, ('m', '?'): 7, ('f', 'h'): 3, ('p', 'o'): 8, ('o', 'u'): 4, ('I', 'V'): 1, ('V', '.'): 2, ('v', 'ö'): 1, ('S', 'o'): 22, ('T', 'ö'): 3, ('T', 'i'): 2, ('p', 'ä'): 13, ('k', 'p'): 7, ('K', 'r'): 2, ('R', 'i'): 1, ('A', 'f'): 2, ('i', 'a'): 8, ('l', 'g'): 7, ('n', 'j'): 3, ('Ö', 'g'): 6, ('Ä', 'n'): 4, ('T', 'v'): 2, ('u', 'c'): 7, ('E', 'g'): 1, ('h', ','): 2, ('U', 's'): 1, ('a', 'b'): 17, ('a', '!'): 2, ('H', 'ä'): 15, ('s', 'd'): 9, ('y', 'ö'): 1, ('ä', 'b'): 2, ('E', 'j'): 1, ('E', 'f'): 2, ('T', 'o'): 2, ('N', 'y'): 1, ('P', 'r'): 5, ('L', 'j'): 3, ('S', 'l'): 2, ('y', 'x'): 1, ('x', 'h'): 1, ('f', 'm'): 2, ('j', 's'): 2, ('R', 'u'): 2, ('ä', 'h'): 3, ('C', 'a'): 2, ('S', 'a'): 2, ('o', '.'): 8, ('A', 'c'): 3, ('d', '!'): 2, ('a', ';'): 1, ('k', 'b'): 4, ('g', '!'): 4, ('R', 'ö'): 1, ('B', 'r'): 1, ('y', 'a'): 3, ('å', 'b'): 2, ('ä', ','): 1, ('f', 'j'): 3, ('d', 'y'): 6, ('M', 'o'): 11, ('p', 'y'): 1, ('k', 'å'): 5, ('t', 'h'): 3, ('B', 'å'): 1, ('F', 'o'): 2, ('u', '!'): 2, ('B', 'ä'): 2, ('l', 'é'): 7, ('é', 'n'): 6, ('K', 'l'): 2, ('A', 'n'): 30, ('G', 'å'): 3, ('u', '?'): 7, ('e', ';'): 1, ('F', 'e'): 1, ('p', 'k'): 3, ('E', 't'): 2, ('V', 'I'): 3, ('D', 'ö'): 5, ('p', 'ö'): 2, ('y', 'i'): 1, ('e', '’'): 1, ('’', 'n'): 1, ('o', 'o'): 1, ('j', 't'): 1, ('y', 'u'): 1, ('s', '-'): 1, ('-', 'k'): 1, ('D', 'i'): 2, ('o', 'e'): 1, ('n', ':'): 4, ('Å', 'n'): 4, ('S', 'ä'): 3, ('å', 'p'): 1, ('k', 'k'): 1, ('a', ':'): 2, ('é', 'e'): 1, ('Å', 't'): 2, ('S', 'k'): 4, ('V', 'o'): 1, ('Å', ','): 1, ('R', 'ä'): 1, ('U', 'r'): 1, ('i', '.'): 5, ('H', 'y'): 2, ('i', '!'): 1, ('K', 'n'): 1, ('P', 'i'): 3, ('å', '?'): 2, ('å', '!'): 1, ('s', '!'): 2, ('ö', 'h'): 1, ('d', 'p'): 1, ('y', 'o'): 1, ('I', 'c'): 2, ('T', 'y'): 4, ('I', 's'): 2, ('p', 'v'): 1, ('K', 'ö'): 1, ('p', '?'): 2, ('U', 't'): 3, ('I', 'X'): 1, ('X', '.'): 2, ('e', ':'): 2, ('K', 'i'): 1, ('P', 'a'): 2, ('j', '?'): 2, ('N', 'o'): 1, ('p', '!'): 1, ('K', 'ä'): 1, ('g', 'b'): 4, ('A', 's'): 2, ('O', 'l'): 8, ('»', 'L'): 2, ('n', '»'): 1, ('»', ','): 1, ('J', 'e'): 1, ('t', ':'): 1, ('p', 'j'): 2, ('v', 'é'): 2, ('é', 'r'): 3, ('å', 'm'): 3, ('x', ','): 1, ('o', 'å'): 1, ('i', 'u'): 1, ('c', 'e'): 2, ('u', 'e'): 2, ('t', '»'): 1, ('»', '.'): 1, ('j', 'k'): 1, ('o', 'z'): 1, ('z', 'a'): 1, ('g', 'y'): 1, ('M', 'ö'): 5, ('n', 'é'): 1, ('h', '!'): 1, ('h', '?'): 1, ('m', ':'): 1, ('t', 'p'): 1, ('g', ':'): 1, ('M', 'å'): 1}\n"
     ]
    }
   ],
   "source": [
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the most frequent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = max(pairs.values())\n",
    "most_freq_pair = [bigram for bigram in pairs.keys() if pairs[bigram] == max_val]\n",
    "most_freq_pair = most_freq_pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d', 'e')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_freq_pair(pairs):\n",
    "    max_val = max(pairs.values())\n",
    "    most_freq_pair = [bigram for bigram in pairs.keys() if pairs[bigram] == max_val]\n",
    "    return most_freq_pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(most_freq_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The First Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the initial symbols in a `vocabulary` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = initial_vocabulary(corpus_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your most frequent pair to the vocabulary after one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.append(''.join(most_freq_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental Construction\n",
    "We will now incrementally build the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `merge_bigrams()` function that takes a list of tokens, `corpus_l`, and a pair of subword tokens `(token_r, token_l)` as input and merges adjacent sequences token_r, token_l into a new token, `token_new`, replacing the sequence `token_r, token_l` in `corpus_l`. Your function will return a new list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input \n",
    "\n",
    "`corpus_l = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', ...]`\n",
    "\n",
    "`merge_bigrams(corpus_l, ('e', 'n'))` should return where all the seuquences of 'e' and 'n' have been merged:\n",
    "\n",
    "`['D', 'e', ' ', 's', 'en', 'a', 's', 't', ...]`\n",
    "\n",
    "And reapplying `merge_bigrams(corpus_l, ('s', 'en'))` to this corpus should return\n",
    "\n",
    "`['D', 'e', ' ', 'sen', 'a', 's', 't', ...]`\n",
    "\n",
    "You will apply a greedy algorithm. Given the pair ('a', 'a') and the list ['a', 'a', 'a'], the result will be: ['aa', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bigrams(corpus, tokens):\n",
    "    seq = ''.join(tokens)\n",
    "    \n",
    "    for index, word in enumerate(corpus):\n",
    "        if index < len(corpus)-1:\n",
    "            if corpus[index] == tokens[0] and corpus[index+1] == tokens[1]:\n",
    "                corpus[index] = seq\n",
    "                corpus.pop(index+1)\n",
    "    return corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'e', ' ', 's', 'en', 'a', 's', 't']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't']\n",
    "merge_bigrams(corpus_test, ('e', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'e', ' ', 'sen', 'a', 's', 't']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_bigrams(merge_bigrams(corpus_test, ('e', 'n')), ('s', 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Pair Encoding (BPE): Building the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write now a `BPE()` function following Algorithm 1 in _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ by Bostrom and Durrett (2020). \n",
    "\n",
    "Your function will take `corpus_l` and the vocabulary size `k` as input. This size `k` will correspond to the count of new subwords added to the initial list of symbols. With your initial corpus, you should have 67 found symbols. With `k = 10`, you will add 10 subwords to this initial list. Note that Bostrom and Durrett (2020) define their $k_\\text{Bostrom and Durrett}$ as `k + initial vocabulary`. \n",
    "\n",
    "Return the vocabulary of subword tokens in the form of a list: the initial vocabulary and the subwords you will create.\n",
    "\n",
    "You will start from the initial vocabulary and `k` will be the number of symbols you add to this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPE(corp: list, k: int)-> list:\n",
    "    vocabulary_corp = initial_vocabulary(corp)\n",
    "    vocab_len = len(vocabulary_corp) \n",
    "    while len(vocabulary_corp) < k + vocab_len:\n",
    "        pairs = pair_count(corp) \n",
    "        most_freq_pair = sorted(pairs, key=lambda x: (-pairs[x], x))[0]\n",
    "        corp = merge_bigrams(corp, most_freq_pair) # update corp with merged most freq pair\n",
    "        vocabulary_corp.append(''.join(most_freq_pair))\n",
    "        print(''.join(most_freq_pair), end=' ') \n",
    "    return vocabulary_corp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a vocabulary of 50 subwords in addition to our initial set of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, encoding='utf8') as f:\n",
    "    corpus = f.read().strip()\n",
    "corpus = re.sub(r'\\s+', ' ', corpus)\n",
    "corpus_l = list(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de en an tt ar st om on ll ör att ch ade ig er ng och var hon et för sk är ck han or na det ne så än in ej un ill den som fv på ed ag li enne henne id ra hade all ing ta "
     ]
    },
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '1',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'X',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '»',\n",
       " 'Ä',\n",
       " 'Å',\n",
       " 'Ö',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'é',\n",
       " 'ö',\n",
       " '–',\n",
       " '’',\n",
       " 'de',\n",
       " 'en',\n",
       " 'an',\n",
       " 'tt',\n",
       " 'ar',\n",
       " 'st',\n",
       " 'om',\n",
       " 'on',\n",
       " 'll',\n",
       " 'ör',\n",
       " 'att',\n",
       " 'ch',\n",
       " 'ade',\n",
       " 'ig',\n",
       " 'er',\n",
       " 'ng',\n",
       " 'och',\n",
       " 'var',\n",
       " 'hon',\n",
       " 'et',\n",
       " 'för',\n",
       " 'sk',\n",
       " 'är',\n",
       " 'ck',\n",
       " 'han',\n",
       " 'or',\n",
       " 'na',\n",
       " 'det',\n",
       " 'ne',\n",
       " 'så',\n",
       " 'än',\n",
       " 'in',\n",
       " 'ej',\n",
       " 'un',\n",
       " 'ill',\n",
       " 'den',\n",
       " 'som',\n",
       " 'fv',\n",
       " 'på',\n",
       " 'ed',\n",
       " 'ag',\n",
       " 'li',\n",
       " 'enne',\n",
       " 'henne',\n",
       " 'id',\n",
       " 'ra',\n",
       " 'hade',\n",
       " 'all',\n",
       " 'ing',\n",
       " 'ta']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = BPE(corpus_l, 50) \n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use the vocabulary you obtained to tokenize a text stored in the corpus string.\n",
    "\n",
    "You will implement a greedy technique building on Python's regular expression engine. You will call this function `tokenize_bpe()` that will take two inputs: `corpus` and `vocabulary`, and that will return the tokenized text in the form of a list.\n",
    "\n",
    "    def tokenize_bpe(corpus, vocabulary):\n",
    "\n",
    "      ...\n",
    "\n",
    "      return tokens\n",
    "Here are a few hints on how to write this function. Before you call a regular expression and apply it to a text, a regex engine compiles it into an efficient automaton (you do not need to call `compile()` as the automaton is automatically cached). The only thing you have to take care of is the length order of the strings. In the tokenization function:\n",
    "\n",
    "1. Write a statement to order the strings in your vocabulary list,\n",
    "  * first by decreasing length, and then\n",
    "  * by alphabetic order.\n",
    "  \n",
    "  You will call this list `vocabulary_srt`; Knowing that, in the ASCII order, the upper case letters are placed before lower case ones, the list: ['D', 'e', 'sen', 'a', 's', 't']\n",
    "\n",
    "will be sorted as: ['sen', 'D', 'a', 'e', 's', 't']\n",
    "\n",
    "2. Escape the regular expression with `re.escape()` as some strings may include metacharacters, for instance 'a.', where the dot matches all the characters.\n",
    "3. Convert this list into a regular expression that results in a disjunction of subword tokens. Remember that the disjunction operator (or) for regular expressions is the vertical bar (`|`), as in `'a'|'b'`, meaning match `'a'` or `'b'`;\n",
    "3. Apply a regular expression function to tokenize your text: the corpus string. You will use `findall()`for this. You will return this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bpe(corpus, vocabulary):\n",
    "    vocabulary_str = sorted(vocabulary, key=lambda x: (-len(x), x)) #sortera enl 1\n",
    "    regex_l = [re.escape(w) for w in vocabulary_str]\n",
    "    tokens = re.findall('|'.join(regex_l), corpus)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'e', 'l', 'm', 'a', 'L', 'ag', 'er', 'l', 'ö', 'f', 'E', 'n', 'h', 'er', 'r', 'g', 'å', 'r', 'd', 's', 's', 'ä', 'g', 'en', 'B', 'o', 'k', 'u', 't', 'g', 'å', 'v', 'a', 'A', 'l', 'b', 'er', 't', 'B', 'on', 'n', 'i', 'er', 's', 'för', 'l', 'ag', ',', 'S', 't', 'o', 'ck', 'h', 'o', 'l', 'm', '1', '8', '9', '9', '.', 'I', '.', 'D', 'et', 'var', 'en', 'sk', 'ö', 'n', 'h', 'ö', 'st', 'd', 'ag', 'h', 'än', 'e', 'm', 'o', 't', 's', 'l', 'u', 't', 'et', 'a', 'f', 't', 'r', 'et', 't', 'i', 'o', 'ta', 'l', 'et', '.', 'P', 'å', 'den', 't', 'id', 'en', 'f', 'an', 'n', 's', 'i', 'U', 'p', 's', 'a', 'l', 'a', 'et', 't', 'h', 'ö', 'g', 't', ',', 'g', 'u', 'l', 't', 't', 'v', 'å', 'v', 'å', 'n', 'ing', 's', 'h', 'u', 's', ',', 'som', 'st', 'o', 'd', 'un', 'de', 'r', 'li', 'g', 't', 'en', 's', 'a', 'm', 't', 'på', 'en', 'li', 't', 'en', 'än', 'g', ',', 'l', 'å', 'ng', 't', 'b', 'or', 'ta', 'i', 'en', 'u', 't', 'k', 'an', 't', 'a', 'f', 'st', 'ade', 'n', '.', 'D', 'et', 'var', 'et', 't', 'r', 'ä', 'tt', 'r', 'u', 'sk', 'ig', 't', 'och', 'o', 't', 'r', 'e']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_bpe(corpus, vocabulary)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now done with BPE and you can now consider the unigram language model.\n",
    "\n",
    "Read these two sections:\n",
    "\n",
    "1. Section 3.2 of _Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates_ (https://arxiv.org/pdf/1804.10959.pdf) by Kudo (2018).\n",
    "2. Section 2, algorithm 2 and the related text of _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ (https://aclanthology.org/2020.findings-emnlp.414.pdf) by Bostrom and Durrett (2020).\n",
    "\n",
    "In your report, **summarize** (10 to 15 lines or so) with your own words the tokenization with a unigram language model as described by by Kudo (2018) and Bostrom and Durrett (2020). You will notably consider two aspects:\n",
    "1. How to obtain the subword vocabulary;\n",
    "2. How to tokenize a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, given what you have done on the byte-pair encoding, how would you build the “reasonably big seed vocabulary” needed for the unigram language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the “reasonably big seed vocabulary”, you will now fit a unigram language model. You will start with a vocabulary of 50 subwords in addition to the character set and reduce it to 49, i.e. you will find one subword to discard.\n",
    "\n",
    "Kudo (2018) proposes the expectation-maximization algorithm that we have not seen in the course on natural language processing. Instead, in this lab, you will approximate the language model with the BPE algorithm.\n",
    "\n",
    "Write a `unigram_lm()` function that takes a corpus string and a vocabulary of subword tokens as input and returns a dictionary, where the keys are the subwords and each key value, the key relative frequency:\n",
    "\n",
    "    def unigram_lm(corpus, vocabulary):\n",
    "\n",
    "       ...\n",
    "\n",
    "      return unigram_probs\n",
    "Your function will:\n",
    "\n",
    "1. Tokenize your corpus with BPE (you can reuse the `tokenize_bpe()` function);\n",
    "2. Estimate the probability of each word (simply count the occurrences of the subwords and divide them by the length of the tokenized corpus);\n",
    "3. Return this model as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_lm(corpus, vocabulary):\n",
    "    # 1. Tokenize corpus with BPE\n",
    "    tokens = tokenize_bpe(corpus, vocabulary)\n",
    "    corpus_size = len(tokens)\n",
    "    print(corpus_size)\n",
    "    # 2. Estimate probability of each word (subword occ)/(tok corpus len)\n",
    "    unigram_probs = {}\n",
    "    for token in tokens:\n",
    "        if token in unigram_probs.keys():\n",
    "            unigram_probs[token] += 1/corpus_size\n",
    "        else:\n",
    "             unigram_probs[token] = 1/corpus_size\n",
    "    return unigram_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'S': 0.0017218346445005243,\n",
       " 'e': 0.027727475137300544,\n",
       " 'l': 0.02868734847360252,\n",
       " 'm': 0.028835782494680145,\n",
       " 'a': 0.040186037306419016,\n",
       " 'L': 0.0003760328533966649,\n",
       " 'ag': 0.004017614170501222,\n",
       " 'er': 0.010360694671218402,\n",
       " 'ö': 0.010083617831873502,\n",
       " 'f': 0.020790658552272863,\n",
       " 'E': 0.00026718123793973564,\n",
       " 'n': 0.017257928850625385,\n",
       " 'h': 0.015773588639849134,\n",
       " 'r': 0.0313195784473791,\n",
       " 'g': 0.031289891643163564,\n",
       " 'å': 0.02125575181831609,\n",
       " 'd': 0.02949878778882687,\n",
       " 's': 0.038038691801495286,\n",
       " 'ä': 0.01375488595319343,\n",
       " 'en': 0.023838503785066766,\n",
       " 'B': 0.0008114393152243838,\n",
       " 'o': 0.015832962248280184,\n",
       " 'k': 0.02452130028202384,\n",
       " 'u': 0.020948988174755663,\n",
       " 't': 0.04488644797387879,\n",
       " 'v': 0.014625698876848831,\n",
       " 'A': 0.0006234228885260504,\n",
       " 'b': 0.015100687744297231,\n",
       " 'on': 0.005719657612191323,\n",
       " 'i': 0.019652664390677737,\n",
       " 'för': 0.009074266488545651,\n",
       " ',': 0.027133739052990043,\n",
       " 'ck': 0.007857107515709125,\n",
       " '1': 9.8956014051754e-06,\n",
       " '8': 9.8956014051754e-06,\n",
       " '9': 1.97912028103508e-05,\n",
       " '.': 0.022156251546187014,\n",
       " 'I': 0.0031171144426302617,\n",
       " 'D': 0.005492058779872298,\n",
       " 'et': 0.011805452476373953,\n",
       " 'var': 0.009885705803770002,\n",
       " 'sk': 0.0082628271733213,\n",
       " 'st': 0.016149621493245784,\n",
       " 'än': 0.00733264064123485,\n",
       " 'ta': 0.0071743110187520494,\n",
       " 'P': 0.00023749443372420947,\n",
       " 'den': 0.005205086339122223,\n",
       " 'id': 0.004680619464647947,\n",
       " 'an': 0.01444757805155568,\n",
       " 'U': 0.00025728563653456025,\n",
       " 'p': 0.012092424917124029,\n",
       " 'ing': 0.0037405373311563146,\n",
       " 'som': 0.005363415961605023,\n",
       " 'un': 0.006639948542872599,\n",
       " 'de': 0.015160061352728281,\n",
       " 'li': 0.008876354460442151,\n",
       " 'på': 0.005165503933501523,\n",
       " 'ng': 0.004512394240759972,\n",
       " 'or': 0.0073128494384245,\n",
       " 'ade': 0.006540992528820849,\n",
       " 'tt': 0.005036861115234248,\n",
       " 'ig': 0.007896689921329825,\n",
       " 'och': 0.010736727524615053,\n",
       " 'det': 0.007342536242640025,\n",
       " 'na': 0.00848053040423515,\n",
       " 'in': 0.005986838850131048,\n",
       " 'x': 0.00040571965761219107,\n",
       " 'är': 0.0081044975508385,\n",
       " 'så': 0.005887882836079298,\n",
       " 'att': 0.012597100588787954,\n",
       " 'all': 0.003968136163475347,\n",
       " 'om': 0.010568502300727077,\n",
       " 'ra': 0.005818613626243073,\n",
       " 'fv': 0.005581119192518873,\n",
       " 'H': 0.007441492256691775,\n",
       " 'ar': 0.008312305180347175,\n",
       " 'ed': 0.005976943248725873,\n",
       " 'han': 0.00733264064123485,\n",
       " 'ill': 0.005116025926475648,\n",
       " 'ne': 0.002523378358319735,\n",
       " 'hade': 0.004017614170501222,\n",
       " 'y': 0.0072435802285882745,\n",
       " 'll': 0.006263915689475949,\n",
       " 'hon': 0.010113304636089027,\n",
       " '–': 0.005501954381277473,\n",
       " 'j': 0.009707584978476852,\n",
       " '?': 0.001177576567215875,\n",
       " 'Å': 0.000455197664638068,\n",
       " 'J': 0.001177576567215875,\n",
       " 'ej': 0.005709762010786148,\n",
       " ':': 0.0001187472168621048,\n",
       " 'G': 0.0006729008955519276,\n",
       " 'F': 0.0005640492800949977,\n",
       " 'ör': 0.003958240562070172,\n",
       " 'M': 0.002454109148483507,\n",
       " 'T': 0.0002968680421552618,\n",
       " 'V': 0.0005244668744742959,\n",
       " '!': 0.0006234228885260504,\n",
       " 'O': 0.001711939043095349,\n",
       " 'ch': 0.0014744446093711383,\n",
       " 'enne': 0.0001187472168621048,\n",
       " '»': 4.9478007025877e-05,\n",
       " 'N': 0.0006531096927415767,\n",
       " 'henne': 0.004453020632328922,\n",
       " 'K': 0.00034634604918113874,\n",
       " 'c': 7.91648112414032e-05,\n",
       " 'C': 2.9686804215526198e-05,\n",
       " 'z': 1.97912028103508e-05,\n",
       " 'Ä': 0.0001187472168621048,\n",
       " ';': 2.9686804215526198e-05,\n",
       " '_': 0.0013458017911038575,\n",
       " 'Ö': 6.92692098362278e-05,\n",
       " 'R': 0.00014843402107763098,\n",
       " 'é': 9.8956014051754e-05,\n",
       " '’': 9.8956014051754e-06,\n",
       " '-': 9.8956014051754e-06,\n",
       " 'X': 1.97912028103508e-05}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_probs = unigram_lm(corpus, vocabulary)\n",
    "unigram_probs # ta och ra försvinner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now apply your unigram language model to tokenize a character sequence that does not include spaces, typically a single word in the Latin or Greek scripts or a sequence of words in Asian scripts, like Chinese or Korean.\n",
    "\n",
    "Write a `tokenize_lm()` function that takes a character sequence, `char_seq`, and a dictionary of unigram probabilities, `unigram_probs`,  as input and returns the subword tokens and the segmentation probability, (prob,tokens). You will only return the token list with the highest probability.\n",
    "\n",
    "    def tokenize_lm(char_seq, unigram_probs):\n",
    "\n",
    "      ...\n",
    "\n",
    "      return max(candidates)\n",
    "\n",
    "As an example, applying \n",
    "\n",
    "tokenize_lm('senare', unigram_probs)\n",
    "results in\n",
    "\n",
    "`(2.0899522820189735e-07, ['s', 'en', 'ar', 'e'])`\n",
    "\n",
    "Your function will cache (memoize) the results to speed up the computation. It will be similar to that of Norvig's in the notebook: How to Do Things with Words.ipynb. You can reuse it.\n",
    "Python has a built-in memoization function that you can use: @functools.lru_cache(maxsize=2**10). You can also use the newer @functools.cache() function if you have Python 3.9 or higher. See here: https://docs.python.org/3/library/functools.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "def tokenize_lm(char_seq, unigram_probs): \n",
    "    # Use one of the two cache functions below to have a faster answer: \n",
    "    # # @functools.lru_cache(maxsize=2**10) \n",
    "    @functools.cache # Available from Python 3.9 \n",
    "    # The arguments of the cached function must be hashable that's why we define an inner cacheable function \n",
    "    def __tokenize_lm(char_seq): \n",
    "        # Write your code here \n",
    "        if not char_seq: \n",
    "            return 1.0, [] \n",
    "        splits = [(char_seq[:i + 1], char_seq[i + 1:]) for i in range(len(char_seq))] \n",
    "        candidates = [] \n",
    "        for first, rest in splits: \n",
    "            first_prob = unigram_probs.get(first, 0.0) \n",
    "            rest_prob, rest = __tokenize_lm(rest) \n",
    "            candidates.append((first_prob * rest_prob, [first] + rest)) \n",
    "        return max(candidates) \n",
    "    return __tokenize_lm(char_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0899522820188539e-07, ['s', 'en', 'ar', 'e'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_lm('senare', unigram_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenization with Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function applies to a sequence without spaces. You will now apply it to your corpus. Write a `tokenize_text_lm()` function that takes the whole `corpus` string as input and the unigram probabilities `unigram_probs` and return the corpus probability and the tokenized subwords. \n",
    "\n",
    "This function is just an application of the functions you just wrote, where you will:\n",
    "1. `split()` the string by whitespaces\n",
    "2. Break the tokens into subtokens and compute the probabilities of the resulting sequences;\n",
    "3. Sum the logarithm of these probabilities. Use log10 to check your output with the numbers in the notebook. \n",
    "\n",
    "It is very significant that you use the logarithm of the probabilities and the sum. If you multiply the probabilities, you will get an underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_lm(corpus, unigram_probs):\n",
    "    # 1. split the corpus by whitespace\n",
    "    corpus = corpus.split()\n",
    "    #corpus = ['_' + s for s in corpus]\n",
    "    # 2. break the tokens into subtokens\n",
    "    subtokens = []\n",
    "    init_loglikelihood = 0\n",
    "    for word in corpus:\n",
    "        # 3. compute probabilities of the resulting sequences\n",
    "        prob_and_seq = tokenize_lm(word, unigram_probs)\n",
    "        subtokens += prob_and_seq[1]\n",
    "       \n",
    "         # 4. sum the logarithm of these probabilities. log10\n",
    "        #try: # valueerror for log10\n",
    "        # print(prob_and_seq[0])\n",
    "        init_loglikelihood += math.log10(prob_and_seq[0])           # TODO 3\n",
    "        #except ValueError:\n",
    "         #   init_loglikelihood += 0\n",
    "    return init_loglikelihood, subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_loglikelihood, tokens = tokenize_text_lm(corpus, unigram_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-183398.97775568598, ['S', 'e', 'l', 'm', 'a', 'L', 'ag', 'er', 'l', 'ö'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loglikelihood, tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement the final loop, where you will, at each iteration:\n",
    "1. Select one subword from the vocabulary.\n",
    "2. Compute the resulting log-likelihood of the corpus without this word.\n",
    "3. Compute the loss, i.e. the log-likelihood reduction when the subword is removed from the current vocabulary\n",
    "\n",
    "You will always keep the single characters in your vocabulary to avoid unknown words.\n",
    "\n",
    "Store the pairs, (log-likelihood, removed_subword) in a list `logloss_word` and rank them by likelihood value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. select one subword from vocabulary\n",
    "\n",
    "\n",
    "    ### find and remove all words in corpus with subword in it\n",
    "    #r1 = \"(\\p{L}*\"  + subw + \"\\p{L}*)\"\n",
    "   # c_without_subw1 = re.sub(r1, '', c)\n",
    "\n",
    "    ### find and remove all subword from words \n",
    "    #r2 = \"(\"  + subw + \")\"\n",
    "    #c_without_subw2 = re.sub(r2, '', c)\n",
    "\n",
    "    ### find and remove all words equal to subword surrounded by spaces\n",
    "    #r3 = '\\s+' + subw + '\\s+'\n",
    "    #c_without_subw3 = re.sub(r3, ' ', c)\n",
    "    #print(subw)\n",
    "    #print(c_without_subw)\n",
    "    \n",
    "    # calculate loglikelihood when words containing subword is removed\n",
    "    \n",
    "    #without_sub_loglikelihood, t = tokenize_text_lm(c_without_subw2, unigram_probs) #TODO 2\n",
    "    #logloss_word.append((init_loglikelihood-without_sub_loglikelihood, subw))\n",
    "    #c = corpus\n",
    "#logloss_word = sorted(logloss_word, key=lambda x: x[0]) # sort by loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de en an tt ar st om on ll ör att ch ade ig er ng och var hon et för sk är ck han or na det ne så än in ej un ill den som fv på ed ag li enne henne id ra hade all ing ta "
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FILE_PATH, encoding='utf8') as f:\n",
    "    corpus = f.read().strip()\n",
    "corpus = re.sub(r'\\s+', ' ', corpus)\n",
    "corpus_l = list(corpus)\n",
    "vocabulary = BPE(corpus_l, 50) \n",
    "vocabulary\n",
    "\n",
    "v = []\n",
    "for c in vocabulary:\n",
    "    if len(c) > 1:\n",
    "        v.append(c)\n",
    "len(v)\n",
    "#print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102434\n",
      "103407\n",
      "102498\n",
      "101490\n",
      "101796\n",
      "102573\n",
      "102123\n",
      "101612\n",
      "101682\n",
      "101364\n",
      "102328\n",
      "101204\n",
      "101712\n",
      "101853\n",
      "102078\n",
      "101511\n",
      "102140\n",
      "102054\n",
      "102077\n",
      "101919\n",
      "101972\n",
      "101890\n",
      "101815\n",
      "101849\n",
      "101796\n",
      "101756\n",
      "101667\n",
      "101797\n",
      "101112\n",
      "101650\n",
      "101642\n",
      "101581\n",
      "101632\n",
      "101615\n",
      "101572\n",
      "101581\n",
      "101597\n",
      "101593\n",
      "101577\n",
      "101539\n",
      "101461\n",
      "101507\n",
      "101067\n",
      "101505\n",
      "101502\n",
      "101505\n",
      "101461\n",
      "101456\n",
      "101433\n",
      "101558\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_copy = vocabulary\n",
    "logloss_word = []\n",
    "for subw in v:\n",
    "    vocab_copy.remove(subw)\n",
    "    # beräkna unigram probs\n",
    "    new_unigram_probs = unigram_lm(corpus, vocab_copy)\n",
    "    #tokenize_text_lm med nya unigram probs\n",
    "    without_sub_loglikelihood, t = tokenize_text_lm(corpus, new_unigram_probs) #TODO 2\n",
    "    logloss_word.append((init_loglikelihood-without_sub_loglikelihood, subw))\n",
    "    vocab_copy.append(subw)\n",
    "logloss_word = list(sorted(logloss_word, key=lambda x: x[0])) # sort by loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-92.75720751026529, 'tt'),\n",
       " (-63.05992010625778, 'ne'),\n",
       " (-38.08057148713851, 'ta'),\n",
       " (2.073845406557666, 'enne'),\n",
       " (12.061332145007327, 'ag'),\n",
       " (20.0173894217005, 'ra'),\n",
       " (71.24347190675326, 'li'),\n",
       " (117.21043151579215, 'll'),\n",
       " (133.63334921753267, 'ed'),\n",
       " (169.48387613944942, 'id'),\n",
       " (172.16923521805438, 'ör'),\n",
       " (178.64230153048993, 'ng'),\n",
       " (192.93231688195374, 'ar'),\n",
       " (203.4787342007039, 'så'),\n",
       " (227.72444155588164, 'na'),\n",
       " (251.89493320829934, 'ch'),\n",
       " (263.8993440794002, 'all'),\n",
       " (264.87091455829795, 'ing'),\n",
       " (283.41390828895965, 'den'),\n",
       " (307.6344156647974, 'som'),\n",
       " (327.04498431726824, 'ade'),\n",
       " (337.89457691146526, 'sk'),\n",
       " (340.59077298568445, 'in'),\n",
       " (341.175971763354, 'det'),\n",
       " (367.5423861454474, 'fv'),\n",
       " (368.11862880166154, 'un'),\n",
       " (379.3587886237365, 'et'),\n",
       " (379.7334326033306, 'hade'),\n",
       " (385.85791648199665, 'på'),\n",
       " (424.2208616445714, 'or'),\n",
       " (435.2485914657882, 'ig'),\n",
       " (437.26840572149376, 'ej'),\n",
       " (442.54784144842415, 'on'),\n",
       " (514.6311576675798, 'än'),\n",
       " (526.9518980000576, 'ill'),\n",
       " (527.494399919291, 'är'),\n",
       " (530.3054780288076, 'er'),\n",
       " (587.395614473935, 'st'),\n",
       " (618.3174647500273, 'han'),\n",
       " (756.8352763502044, 'henne'),\n",
       " (758.2914515881566, 'de'),\n",
       " (791.0982600485149, 'om'),\n",
       " (1052.5372796481824, 'an'),\n",
       " (1112.5860845244315, 'för'),\n",
       " (1156.332144005195, 'var'),\n",
       " (1210.4481459772505, 'ck'),\n",
       " (1270.4310848150635, 'hon'),\n",
       " (1278.6004571144294, 'att'),\n",
       " (1626.1150182384008, 'och'),\n",
       " (2125.6897525669774, 'en')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_word # TODO 1 what is wrong in creation of logloss_word and how to act with log10 to not throw error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will reduce now your vocabulary by one token: `out_candidate`. Write the piece of code to determine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# THE TOKEN WITH THE LOWEST LOGLIKELIHOOD SHOULD BE REMOVED \n",
    "out_candidate = logloss_word[0][1]\n",
    "vocabulary.remove(out_candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tt'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can improve this program and test it on larger corpora. You can also read a fine implementation of BPE by Andrej Karpathy: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"fi6368ha-s\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"5-BPE.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the subword to discard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"out_candidate\": \"tt\"}'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'out_candidate': out_candidate})\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 5\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': '27658aa15572d6006c0d2823fb0e034ba962ecd0c79eb742e42b2ee57d20bab0de6336ab6fda6eec7a3c151421c7119d2931c5671f68dc499459519bfe12cac7',\n",
       " 'submission_id': '56eb7df9-c19f-4571-8b59-83eb2cdb8e19'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Describe the background as well as the algorithms you used. For this, summarize the articles as described in the notebook:\n",
    "   * Preliminaries: subword tokenizers\n",
    "   * Design of the BPE Algorithm\n",
    "   * Unigram Language Model\n",
    "2. Describe your program as well as your results\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (www.overleaf.com). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 14, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can improve this program and test it on larger corpora. You can also read a fine implementation of BPE by Andrej Karpathy: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
